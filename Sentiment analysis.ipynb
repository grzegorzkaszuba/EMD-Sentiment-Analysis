{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Eksploracja masywnych danych </h1>\n",
    "<h2> Projekt Python - analiza sentymentu </h2>\n",
    "\n",
    "<h3> 1. WstÄ™pna analiza danych i preprocessing </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(555791, 9)\n",
      "(555745, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A35C43YE9HU9CN</td>\n",
       "      <td>B0064X7B4A</td>\n",
       "      <td>Joan Miller</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I have decided not to play this game.  I can't...</td>\n",
       "      <td>Friends</td>\n",
       "      <td>1396396800</td>\n",
       "      <td>04 2, 2014</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHFS8CGWWXB5B</td>\n",
       "      <td>B00H1P4V3E</td>\n",
       "      <td>WASH ST. GAMER</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>The Amazon Appstore free app of the day for Ju...</td>\n",
       "      <td>Amazon Makes This \"Longest Spring Ever\" for Fi...</td>\n",
       "      <td>1402272000</td>\n",
       "      <td>06 9, 2014</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3EW8OTQ90NVHM</td>\n",
       "      <td>B00CLVW82O</td>\n",
       "      <td>Kindle Customer</td>\n",
       "      <td>[0, 4]</td>\n",
       "      <td>this game was so mush fun I wish I could play ...</td>\n",
       "      <td>best</td>\n",
       "      <td>1368921600</td>\n",
       "      <td>05 19, 2013</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AJ3GHFJY1IUTD</td>\n",
       "      <td>B007T9WVKM</td>\n",
       "      <td>BrawlMaster4</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>Its pretty fun and very good looking,  but you...</td>\n",
       "      <td>Fun Game</td>\n",
       "      <td>1350172800</td>\n",
       "      <td>10 14, 2012</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3JJGBS4EL603S</td>\n",
       "      <td>B00J206J5E</td>\n",
       "      <td>K. Wilson \"thesupe\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>good graphics; immersive storyline; hard to st...</td>\n",
       "      <td>great game!</td>\n",
       "      <td>1396915200</td>\n",
       "      <td>04 8, 2014</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A3RL7Y2FJBDHJ0</td>\n",
       "      <td>B006H7TC3Q</td>\n",
       "      <td>hi</td>\n",
       "      <td>[2, 5]</td>\n",
       "      <td>its very good.u use fotos on ur device and it ...</td>\n",
       "      <td>very good</td>\n",
       "      <td>1337817600</td>\n",
       "      <td>05 24, 2012</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AUHVMC0PURGO8</td>\n",
       "      <td>B006R6VG9K</td>\n",
       "      <td>A.Mccullough</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>the game is very fun and fast paced. It also k...</td>\n",
       "      <td>fun and fast paced</td>\n",
       "      <td>1401926400</td>\n",
       "      <td>06 5, 2014</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A1Z37DUIWXJNLN</td>\n",
       "      <td>B00B63HT8Q</td>\n",
       "      <td>Julie Quick \"Beach Bum Wannabe\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>great app!  A quick look at the weather... not...</td>\n",
       "      <td>A great alternative to the current (stupid) ve...</td>\n",
       "      <td>1377388800</td>\n",
       "      <td>08 25, 2013</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AF7ZE5MRM6CW2</td>\n",
       "      <td>B00BL0I7WG</td>\n",
       "      <td>T.dd</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>So fare I like it haven't had it long enough t...</td>\n",
       "      <td>easy fun</td>\n",
       "      <td>1369612800</td>\n",
       "      <td>05 27, 2013</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A1TTH51E2651BJ</td>\n",
       "      <td>B00GRXA7GG</td>\n",
       "      <td>Joni</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This classic Mahjong comes with nice graphics ...</td>\n",
       "      <td>Mahjong Premium</td>\n",
       "      <td>1394841600</td>\n",
       "      <td>03 15, 2014</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A09537361YDF89V20S3QI</td>\n",
       "      <td>B00DBGXAVG</td>\n",
       "      <td>Shronica</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>I guess its okay. Again my daughter downloaded...</td>\n",
       "      <td>I guess</td>\n",
       "      <td>1396915200</td>\n",
       "      <td>04 8, 2014</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A3PDQ6MSVNAHU5</td>\n",
       "      <td>B0064X7B4A</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>fun to play with friends from all around the w...</td>\n",
       "      <td>better than the board game!</td>\n",
       "      <td>1372377600</td>\n",
       "      <td>06 28, 2013</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AVG4YZ0ZV4F9X</td>\n",
       "      <td>B004Q6Y2LM</td>\n",
       "      <td>CDarbs</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Game ran fine on my Droid- noticed only a slig...</td>\n",
       "      <td>No gameplay problems, but not my cup of tea</td>\n",
       "      <td>1306368000</td>\n",
       "      <td>05 26, 2011</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AM55082AHY624</td>\n",
       "      <td>B008XG1X18</td>\n",
       "      <td>nicholscl</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This is a different way to look at pinterest a...</td>\n",
       "      <td>Love Pinterst on my PC</td>\n",
       "      <td>1379462400</td>\n",
       "      <td>09 18, 2013</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A2ZC0Q9GKBD44E</td>\n",
       "      <td>B008JJS6D2</td>\n",
       "      <td>Caitlin Anne \"Caitling\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This is one of the most fun games, I have play...</td>\n",
       "      <td>fun</td>\n",
       "      <td>1358553600</td>\n",
       "      <td>01 19, 2013</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               reviewerID        asin                     reviewerName  \\\n",
       "0          A35C43YE9HU9CN  B0064X7B4A                      Joan Miller   \n",
       "1           AHFS8CGWWXB5B  B00H1P4V3E                   WASH ST. GAMER   \n",
       "2          A3EW8OTQ90NVHM  B00CLVW82O                  Kindle Customer   \n",
       "3           AJ3GHFJY1IUTD  B007T9WVKM                     BrawlMaster4   \n",
       "4          A3JJGBS4EL603S  B00J206J5E              K. Wilson \"thesupe\"   \n",
       "5          A3RL7Y2FJBDHJ0  B006H7TC3Q                               hi   \n",
       "6           AUHVMC0PURGO8  B006R6VG9K                     A.Mccullough   \n",
       "7          A1Z37DUIWXJNLN  B00B63HT8Q  Julie Quick \"Beach Bum Wannabe\"   \n",
       "8           AF7ZE5MRM6CW2  B00BL0I7WG                             T.dd   \n",
       "9          A1TTH51E2651BJ  B00GRXA7GG                             Joni   \n",
       "10  A09537361YDF89V20S3QI  B00DBGXAVG                         Shronica   \n",
       "11         A3PDQ6MSVNAHU5  B0064X7B4A                  Amazon Customer   \n",
       "12          AVG4YZ0ZV4F9X  B004Q6Y2LM                           CDarbs   \n",
       "13          AM55082AHY624  B008XG1X18                        nicholscl   \n",
       "14         A2ZC0Q9GKBD44E  B008JJS6D2          Caitlin Anne \"Caitling\"   \n",
       "\n",
       "   helpful                                         reviewText  \\\n",
       "0   [0, 0]  I have decided not to play this game.  I can't...   \n",
       "1   [3, 4]  The Amazon Appstore free app of the day for Ju...   \n",
       "2   [0, 4]  this game was so mush fun I wish I could play ...   \n",
       "3   [0, 2]  Its pretty fun and very good looking,  but you...   \n",
       "4   [0, 0]  good graphics; immersive storyline; hard to st...   \n",
       "5   [2, 5]  its very good.u use fotos on ur device and it ...   \n",
       "6   [0, 0]  the game is very fun and fast paced. It also k...   \n",
       "7   [0, 0]  great app!  A quick look at the weather... not...   \n",
       "8   [0, 0]  So fare I like it haven't had it long enough t...   \n",
       "9   [0, 0]  This classic Mahjong comes with nice graphics ...   \n",
       "10  [1, 1]  I guess its okay. Again my daughter downloaded...   \n",
       "11  [0, 0]  fun to play with friends from all around the w...   \n",
       "12  [0, 0]  Game ran fine on my Droid- noticed only a slig...   \n",
       "13  [0, 0]  This is a different way to look at pinterest a...   \n",
       "14  [0, 0]  This is one of the most fun games, I have play...   \n",
       "\n",
       "                                              summary  unixReviewTime  \\\n",
       "0                                             Friends      1396396800   \n",
       "1   Amazon Makes This \"Longest Spring Ever\" for Fi...      1402272000   \n",
       "2                                                best      1368921600   \n",
       "3                                            Fun Game      1350172800   \n",
       "4                                         great game!      1396915200   \n",
       "5                                           very good      1337817600   \n",
       "6                                  fun and fast paced      1401926400   \n",
       "7   A great alternative to the current (stupid) ve...      1377388800   \n",
       "8                                            easy fun      1369612800   \n",
       "9                                     Mahjong Premium      1394841600   \n",
       "10                                            I guess      1396915200   \n",
       "11                        better than the board game!      1372377600   \n",
       "12        No gameplay problems, but not my cup of tea      1306368000   \n",
       "13                             Love Pinterst on my PC      1379462400   \n",
       "14                                                fun      1358553600   \n",
       "\n",
       "     reviewTime  score  \n",
       "0    04 2, 2014    1.0  \n",
       "1    06 9, 2014    2.0  \n",
       "2   05 19, 2013    5.0  \n",
       "3   10 14, 2012    5.0  \n",
       "4    04 8, 2014    5.0  \n",
       "5   05 24, 2012    5.0  \n",
       "6    06 5, 2014    5.0  \n",
       "7   08 25, 2013    4.0  \n",
       "8   05 27, 2013    5.0  \n",
       "9   03 15, 2014    5.0  \n",
       "10   04 8, 2014    3.0  \n",
       "11  06 28, 2013    5.0  \n",
       "12  05 26, 2011    3.0  \n",
       "13  09 18, 2013    3.0  \n",
       "14  01 19, 2013    4.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('reviews_train.csv')\n",
    "print(df.shape)\n",
    "df = df.dropna()\n",
    "print(df.shape)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score\n",
       "1.0    0.108215\n",
       "2.0    0.060702\n",
       "3.0    0.114966\n",
       "4.0    0.209690\n",
       "5.0    0.506427\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_counts = df.value_counts('score')\n",
    "score_frequencies = score_counts/np.sum(score_counts)\n",
    "score_frequencies = score_frequencies.sort_index()\n",
    "score_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Interpretacja danych </h4>\n",
    "\n",
    "WstÄ™pne przejrzenie przykÅ‚adowych danych dostÄ™pnych w dokumencie pozwala na wstÄ™pnÄ… ocenÄ™ ich uÅ¼ytecznoÅ›ci, a takÅ¼e potencjalnych trudnoÅ›ci w ich interpretacji.\n",
    "\n",
    "JuÅ¼ pierwszy rzut oka na dane wskazuje, jak wielkie znaczenie moÅ¼e mieÄ‡ (przewaÅ¼nie krÃ³tkie) podsumowanie obecne w kolumnie \"summary\". NiektÃ³rzy uÅ¼ytkownicy wprost umeszczajÄ… w niej swoje oceny, Å‚atwe do zinterpretowania na kilkustopniowej skali.\n",
    "\n",
    "Najbardziej wiarygodnÄ… podstawÄ… do klasyfikacji, pod warunkiem dobrej jakoÅ›ci modelu, powinien byÄ‡ peÅ‚en tekst recenzji. W przypadku problemu analizy sentymentu, kwestia wykorzystania \"stop words\" jest trudna. Wiele sÅ‚Ã³w (np. zaprzeczeÅ„) sÄ… kluczowe dla prawidÅ‚owej klasyfikacji wypowiedzi i nie naleÅ¼y ich usuwaÄ‡. ByÄ‡ moÅ¼e jednak warto pozbyÄ‡ siÄ™ innych, nie niosÄ…cych takich informacji.\n",
    "\n",
    "Wiele sÅ‚Ã³w wprost odwraca znaczenie dalszej czÄ™Å›ci zdania. Nie sa to tylko trywialne przypadki, jak te z zaprzeczeniem (np. \"not\"). JuÅ¼ w niewielkim wycinku danych widoczny jest taki przykÅ‚ad - \"A great ALTERNATIVE to (...) STUPID...\". Pokazuje to, Å¼e sÅ‚owa, ktÃ³re w ten sposÃ³b dziaÅ‚ajÄ… na interpretacjÄ™ zdania sÄ… powszechne i nietrywialne - odpowiednio dobrany system uczÄ…cy powinien zwracaÄ‡ uwagÄ™ na kolejnoÅ›Ä‡ sÅ‚Ã³w lub choÄ‡by rozrÃ³Å¼niaÄ‡ kolejne zdania. By sprawdziÄ‡, jak waÅ¼na jest ta zdolnoÅ›Ä‡, najpierw zostanie przeprowadzona klasyfikacja z uÅ¼yciem modelu, ktÃ³ry jej nie posiada - naiwnego klasyfikatora bayesowskiego.\n",
    "\n",
    "Wykorzystanie gÅ‚osÃ³w innych uÅ¼ytkownikÃ³w na temat uÅ¼ytecznoÅ›ci opinii moÅ¼e mijaÄ‡ siÄ™ z celem. Na podstawie niewielkej prÃ³bki zdaje siÄ™, Å¼e liczba gÅ‚osÃ³w pod opiniami wynosi zero lub jest niewielka. MoÅ¼na siÄ™ spodziewaÄ‡, Å¼e pozytywne gÅ‚osy zbiorÄ… te recenzjÄ™, ktÃ³rych ocena zgodna jest z ocenÄ… wiÄ™kszoÅ›ci uÅ¼ytkownikÃ³w, a opinie niepopularne zbiorÄ… gÅ‚osy negatywne. Obecne w zbiorze opinie dotyczÄ… jednak rÃ³Å¼nych produktÃ³w, wobec tego prawdopodobnie nie ma sensu Å›ledziÄ‡ tego ogÃ³lnego trendu, by wydobyÄ‡ dodatkowe informacje.\n",
    "\n",
    "Problemem, ktÃ³ry naleÅ¼y mieÄ‡ na uwadze, jest niezbilansowanie zbioru pod wzglÄ™dem ocen. Ponad poÅ‚owa ocen jest rÃ³wna 5, a ponad 70% jest rÃ³wna 4 lub 5. MoÅ¼na wiÄ™c przyjÄ…Ä‡, Å¼e klasyfikatory, ktÃ³re wnoszÄ… pewnÄ… jakoÅ›Ä‡ do klasyfikacji (oczywiÅ›cie z perspektywy trafnoÅ›ci, a nie innych metryk) to takie, ktÃ³re klasyfikujÄ… prawidÅ‚owo wiÄ™cej niÅ¼ 50% przypadkÃ³w.\n",
    "\n",
    "Pierwsze podejÅ›cie do problemu zostanie wykonane w dwÃ³ch wariantach - z predykcjÄ… jednej z 5 ocen lub predykcjÄ…, czy ocena jest pozytywna (4-5) lub negatywna (1-3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A35C43YE9HU9CN</td>\n",
       "      <td>B0064X7B4A</td>\n",
       "      <td>Joan Miller</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I have decided not to play this game.  I can't...</td>\n",
       "      <td>Friends</td>\n",
       "      <td>1396396800</td>\n",
       "      <td>04 2, 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHFS8CGWWXB5B</td>\n",
       "      <td>B00H1P4V3E</td>\n",
       "      <td>WASH ST. GAMER</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>The Amazon Appstore free app of the day for Ju...</td>\n",
       "      <td>Amazon Makes This \"Longest Spring Ever\" for Fi...</td>\n",
       "      <td>1402272000</td>\n",
       "      <td>06 9, 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3EW8OTQ90NVHM</td>\n",
       "      <td>B00CLVW82O</td>\n",
       "      <td>Kindle Customer</td>\n",
       "      <td>[0, 4]</td>\n",
       "      <td>this game was so mush fun I wish I could play ...</td>\n",
       "      <td>best</td>\n",
       "      <td>1368921600</td>\n",
       "      <td>05 19, 2013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AJ3GHFJY1IUTD</td>\n",
       "      <td>B007T9WVKM</td>\n",
       "      <td>BrawlMaster4</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>Its pretty fun and very good looking,  but you...</td>\n",
       "      <td>Fun Game</td>\n",
       "      <td>1350172800</td>\n",
       "      <td>10 14, 2012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3JJGBS4EL603S</td>\n",
       "      <td>B00J206J5E</td>\n",
       "      <td>K. Wilson \"thesupe\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>good graphics; immersive storyline; hard to st...</td>\n",
       "      <td>great game!</td>\n",
       "      <td>1396915200</td>\n",
       "      <td>04 8, 2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin         reviewerName helpful  \\\n",
       "0  A35C43YE9HU9CN  B0064X7B4A          Joan Miller  [0, 0]   \n",
       "1   AHFS8CGWWXB5B  B00H1P4V3E       WASH ST. GAMER  [3, 4]   \n",
       "2  A3EW8OTQ90NVHM  B00CLVW82O      Kindle Customer  [0, 4]   \n",
       "3   AJ3GHFJY1IUTD  B007T9WVKM         BrawlMaster4  [0, 2]   \n",
       "4  A3JJGBS4EL603S  B00J206J5E  K. Wilson \"thesupe\"  [0, 0]   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  I have decided not to play this game.  I can't...   \n",
       "1  The Amazon Appstore free app of the day for Ju...   \n",
       "2  this game was so mush fun I wish I could play ...   \n",
       "3  Its pretty fun and very good looking,  but you...   \n",
       "4  good graphics; immersive storyline; hard to st...   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0                                            Friends      1396396800   \n",
       "1  Amazon Makes This \"Longest Spring Ever\" for Fi...      1402272000   \n",
       "2                                               best      1368921600   \n",
       "3                                           Fun Game      1350172800   \n",
       "4                                        great game!      1396915200   \n",
       "\n",
       "    reviewTime  score  \n",
       "0   04 2, 2014      0  \n",
       "1   06 9, 2014      0  \n",
       "2  05 19, 2013      1  \n",
       "3  10 14, 2012      1  \n",
       "4   04 8, 2014      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_grade_df = df.copy()\n",
    "binary_grade_df['score'] = np.where(binary_grade_df['score'] > 3, 1, 0)\n",
    "binary_grade_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Grzegorz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Grzegorz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have decided not to play this game.  I can't keep track of everyone, or play often enough to make it fun.\n",
      "I have decided not to play this game . I ca n't keep track of everyone , or play often enough to make it fun .\n",
      "I have decided not to play this game . I ca n't keep track of everyone , or play often enough to make it fun .\n",
      "i have decid not to play thi game . i ca n't keep track of everyon , or play often enough to make it fun .\n"
     ]
    }
   ],
   "source": [
    "example_sentence = binary_grade_df.iloc[0,4]\n",
    "tokens = word_tokenize(binary_grade_df.iloc[0,4])\n",
    "tokens_pretty = ' '.join(tokens)\n",
    "lemmas = ' '.join([lemmatizer.lemmatize(t) for t in tokens])\n",
    "stems = ' '.join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "print(example_sentence, tokens_pretty, lemmas, stems, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widaÄ‡ na powyÅ¼szym przykÅ‚adzie, stemming upraszcza tekst nieco bardziej niÅ¼ lematyzacja. Z jednej strony, wpÅ‚ywa to na lepsze uogÃ³lnienie sÅ‚Ã³w, z drugiej - moÅ¼e znieksztaÅ‚ciÄ‡ kontekst zdania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2. WstÄ™pna klasyfikacja </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ze wzglÄ™du na rodzaj problemu (kolejnym etykietom odpowiadajÄ… wartoÅ›ci uporzÄ…dkowane liniowo), oprÃ³cz metryk typowych dla klasyfikacji wieloklasowej, wykorzystane zostanÄ… metryki typowe dla regresji. W poniÅ¼szej kolumnie zdefiniowano uogÃ³lnienie metryk f1 dla klasyfikacji wieloklasowej oraz Å›redni bÅ‚Ä…d absolutny i bÅ‚Ä…d Å›redniokwadratowy, wyliczane z macierzy pomyÅ‚ek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = lambda confusion_mx, clazz: confusion_mx[clazz, clazz]/np.sum(confusion_mx[:, clazz])\n",
    "precision = lambda confusion_mx, clazz: confusion_mx[clazz, clazz]/np.sum(confusion_mx[clazz, :])\n",
    "micro_f1_score = lambda confusion_mx, clazz: 2/(precision(confusion_mx, clazz)**-1 * recall(confusion_mx, clazz)**-1)\n",
    "macro_f1_score = lambda confusion_mx: np.mean([micro_f1_score(confusion_mx, clazz) for clazz in range(confusion_mx.shape[0])])\n",
    "\n",
    "accuracy = lambda confusion_mx: np.sum(np.diag(confusion_mx))/np.sum(confusion_mx)\n",
    "\n",
    "error_matrix = np.abs(np.arange(5)[:, np.newaxis] - np.arange(5))\n",
    "mae = lambda confusion_mx: np.sum(confusion_mx * error_matrix / np.sum(confusion_mx))\n",
    "mse = lambda confusion_mx: np.sum(confusion_mx * error_matrix**2 /  np.sum(confusion_mx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gdyby klasyfikator losowo przypisywaÅ‚ etykiety, ale z prawdopodobieÅ„stwem proporcjonalnym do liczebnoÅ›ci klasy decyzyjnej, wÃ³wczas f1 score wyniÃ³sÅ‚by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13162005051281517"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "dummy_confusion_mx = lambda freqs, probs: freqs[:, np.newaxis] * probs\n",
    "\n",
    "freqs = np.array(score_frequencies)\n",
    "probs = freqs\n",
    "\n",
    "macro_f1_score(dummy_confusion_mx(freqs, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla wygody modelowania wynikÃ³w klasyfikatora niewnoszÄ…cego wartoÅ›ci informacyjnej, a takÅ¼e pÃ³Åºniejszego przetwarzania licznych wynikÃ³w z walidacji jedynie na podstawie macierzy pomyÅ‚ek, zdefiniowano funkcje accuracy i f1_score wyliczane z macierzy pomyÅ‚ek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "reviewText = np.array(df['reviewText'])\n",
    "summaryText = np.array(df['summary'])\n",
    "labels_multi = np.array(df['score'])\n",
    "labels_binary = np.array(binary_grade_df['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> WstÄ™pna optymalizacja modelu </h4>\n",
    "\n",
    "Zaproponowano dwa sposoby wektoryzacji, z wykorzystaniem klasy CountVectorizer (zliczanie sÅ‚Ã³w) oraz TfidfVectorizer (przypisujÄ…cym wyÅ¼szÄ… wagÄ™ za wystÄ…pienie termÃ³w wÃ³wczas, gdy znajdÄ… siÄ™ one w mniejszej liczbie przykÅ‚adÃ³w ze zbioru - realizuje to podobnÄ… ideÄ™, co wycinanie stop words, tylko w bardziej \"miÄ™kki\" sposÃ³b). <br>\n",
    "ZarÃ³wno dla jakoÅ›ci, jak i prÄ™dkoÅ›ci predykcji, konieczne okazaÅ‚o siÄ™ ograniczenie liczby atrybutÃ³w: narzucenie limitu maksymalnej liczby najczÄ™Å›ciej wystÄ™pujÄ…cych sÅ‚Ã³w lub minimalnÄ… liczbÄ™ wystÄ…pieÅ„ sÅ‚owa w zborze danych. W drugim podejÅ›ciu, optymalizacja okazaÅ‚a siÄ™ Å‚atwiejsza. <br>\n",
    "Zgodnie ze wskazÃ³wkami dotyczÄ…cymi problemu analizy sentymentu, lepsze wyniki osiÄ…gniÄ™to w podejÅ›ciu bez usuwania stop words. <br>\n",
    "Ze wzglÄ™du na znaczny koszt tokenizacji masywnego zbioru (nie jest to operacja, ktÃ³rÄ… moÅ¼na zwektoryzowaÄ‡, a wyjÅ›ciowe dane majÄ… typ \"object\"), pozostano przy natywnej tokenizacji wykorzystywanej przez pakiet sklearn.\n",
    "\n",
    "Zoptymalizowany model osiÄ…ga lepsze wyniki niÅ¼ \"Å›lepy\" klasyfikator, preferujÄ…cy zawsze klasÄ™ wiÄ™kszoÅ›ciowÄ…. Niezoptymalizowane modele daleko odbiegaÅ‚y od tych wynikÃ³w - w obecnoÅ›ci zbÄ™dnych, maÅ‚o informatywnych sÅ‚Ã³w, klasyfikacja byÅ‚a znacznie zaburzona i klasa wiÄ™kszoÅ›ciowa nie zawsze nawet tÄ… najczÄ™Å›ciej wybieranÄ… przez model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer1 = CountVectorizer(min_df=250)\n",
    "vectorizer2 = TfidfVectorizer(min_df=250)\n",
    "#vectorizer1 = CountVectorizer(min_df=250, stop_words='english')\n",
    "#vectorizer2 = TfidfVectorizer(min_df=250, stop_words='english')\n",
    "datapoints = 50000\n",
    "xdata1 = vectorizer1.fit_transform(reviewText[:datapoints])\n",
    "xdata2 = vectorizer2.fit_transform(reviewText[:datapoints])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> DobÃ³r vectorizera </h4>\n",
    "\n",
    "Ze wzglÄ™du na znacznÄ… iloÅ›Ä‡ danych, wstÄ™pny wybÃ³r modelu zostaÅ‚ wykonany przez iteracyjny dobÃ³r parametru min_df i porÃ³wnanie dziaÅ‚ania obu klas Vectorizer. PoniewaÅ¼ prÃ³bka 50000 punktÃ³w danych to nieznaczna czÄ™Å›Ä‡ caÅ‚ego zbioru (mniej niÅ¼ 10%), nie wydzielono zbioru testowego. ZbiÃ³r walidacyjny i testowy zostanÄ… rozrÃ³Å¼nione przy finalnej parametryzacji modelu dla caÅ‚ego datasetu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_val1, X_train2, X_val2, y_train, y_val, binary_y_train, binary_y_val = train_test_split(\n",
    "    xdata1, xdata2, df['score'][:datapoints], binary_grade_df['score'][:datapoints], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classifier1 = GaussianNB()\n",
    "binary_classifier2 = GaussianNB()\n",
    "multiclass_classifier1 = GaussianNB()\n",
    "multiclass_classifier2 = GaussianNB()\n",
    "\n",
    "binary_classifier1.fit(X_train1.toarray(), binary_y_train)\n",
    "binary_classifier2.fit(X_train2.toarray(), binary_y_train)\n",
    "multiclass_classifier1.fit(X_train1.toarray(), y_train)\n",
    "multiclass_classifier2.fit(X_train2.toarray(), y_train)\n",
    "\n",
    "y_pred_bin1 = binary_classifier1.predict(X_val1.toarray())\n",
    "y_pred_bin2 = binary_classifier2.predict(X_val2.toarray())\n",
    "y_pred_multi1 = multiclass_classifier1.predict(X_val1.toarray())\n",
    "y_pred_multi2 = multiclass_classifier2.predict(X_val2.toarray())\n",
    " \n",
    "res_bin_1 = confusion_matrix(binary_y_val, y_pred_bin1)\n",
    "res_bin_2 = confusion_matrix(binary_y_val, y_pred_bin2)\n",
    "res_multi_1 = confusion_matrix(y_val, y_pred_multi1)\n",
    "res_multi_2 = confusion_matrix(y_val, y_pred_multi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorizer accuracies:  0.7656 0.4815\n",
      "Count vectorizer accuracies:  0.7485 0.4456\n"
     ]
    }
   ],
   "source": [
    "print('Count vectorizer accuracies: ', accuracy(res_bin_1), accuracy(res_multi_1))\n",
    "print('Count vectorizer accuracies: ', accuracy(res_bin_2), accuracy(res_multi_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer odnosi nieco lepsze rezultaty na zbiorze walidacyjnym. Zostanie wykorzystany w dalszej czÄ™Å›ci eksperymentÃ³w.\n",
    "\n",
    "<h4> UogÃ³lnienie testÃ³w </h4>\n",
    "\n",
    "Ze wzglÄ™du na fakt, Å¼e klasyfikator nie jest w stanie przetwarzaÄ‡ wektorÃ³w rzadkich, dane uczÄ…ce mogÄ… mieÄ‡ rozmiar wielokrotne przekraczajÄ…cy dostÄ™pnÄ… pamiÄ™Ä‡ operacyjnÄ…. ZaletÄ… staje siÄ™ tutaj prostota klasyfikatora - uczy siÄ™ on przez zliczanie, jednorazowo przechodzÄ…c po danych. Wystarczy zatem wykonaÄ‡ jednÄ… iteracjÄ™, przetwarzajÄ…c kolejne fragmenty zbioru na macierz peÅ‚nÄ….\n",
    "\n",
    "Ponadto, schemat kaÅ¼dego eksperymentu z wektoryzacjÄ… i klasyfikacjÄ… tekstÃ³w bÄ™dzie wyglÄ…daÅ‚ podobnie, jak w przykÅ‚adzie powyÅ¼ej. Czas zdefiniowaÄ‡ caÅ‚Ä… procedurÄ™. Potrzebna bÄ™dzie moÅ¼liwoÅ›Ä‡ doboru rÃ³Å¼nych wektoryzacji, rÃ³Å¼nych klasyfikatorÃ³w, rÃ³Å¼nych danych i etykiet (binarnych lub wieloklasowych), a takÅ¼e wybÃ³r miÄ™dzy wektorem danych w postaci peÅ‚nej i rzadkiej. Drobna nadmiarowoÅ›Ä‡ (na przykÅ‚ad kaÅ¼dorazowe wyznaczanie trzech zbiorÃ³w - uczÄ…cego, walidacyjnego i testowego) zostanie zrekompensowana przez wygodÄ™ uÅ¼ycia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(vectorizer, classifier, data, labels, number_of_datapoints=None, batch_size=None, use_test_set=False, transform_sparse_data=False, return_model=False):\n",
    "    \n",
    "    if vectorizer is not None:\n",
    "        # Wektoryzacja - eksperyment na niepeÅ‚nym zbiorze danych\n",
    "        if number_of_datapoints is None:\n",
    "            xdata = vectorizer.fit_transform(data)\n",
    "        # ...na peÅ‚nym zbiorze\n",
    "        else:\n",
    "            xdata = vectorizer.fit_transform(data[:number_of_datapoints])\n",
    "            labels = labels[:number_of_datapoints]\n",
    "    else:\n",
    "        if number_of_datapoints is None:\n",
    "            xdata = data\n",
    "        else:\n",
    "            xdata = vectorizer[:number_of_datapoints]\n",
    "            labels = labels[:number_of_datapoints]\n",
    "        \n",
    "    # deterministyczny podziaÅ‚ na zbiÃ³r uczÄ…cy, walidacyjny i testowy\n",
    "    x_train, x_test, y_train, y_test = train_test_split(xdata, labels, test_size=0.2, random_state=111)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=7)\n",
    "    \n",
    "    # uczenie i walidacja/testowanie\n",
    "    if batch_size is None:\n",
    "        # wymagane przez klasyfikator NB, niewymagane przez regresjÄ™ logistycznÄ…\n",
    "        if transform_sparse_data:\n",
    "            x_train = x_train.toarray()\n",
    "            x_val = x_val.toarray()\n",
    "            x_test = x_test.toarray()\n",
    "        classifier.fit(x_train, y_train)\n",
    "        # ostateczna prÃ³ba bÄ™dzie wykonywana na zbiorze testowym\n",
    "        if use_test_set:\n",
    "            result = confusion_matrix(y_val, classifier.predict(x_val))\n",
    "        # wiÄ™kszoÅ›Ä‡ (domyÅ›lnie) na walidacyjnym\n",
    "        else:\n",
    "            result = confusion_matrix(y_val, classifier.predict(x_val))\n",
    "    # wersja z batch_size potrzebuje etykiet klas do i wykorzystuje metodÄ™ partial_fit\n",
    "    else:\n",
    "        classes = np.unique(y_train)\n",
    "        for i in range(0, y_train.shape[0], batch_size):\n",
    "            if transform_sparse_data:\n",
    "                x_train_batch = x_train[i:i+batch_size].toarray()\n",
    "            else:\n",
    "                x_train_batch = x_train[i:i+batch_size]\n",
    "            classifier.partial_fit(x_train_batch, y_train[i:i+batch_size], classes)\n",
    "        if use_test_set:\n",
    "            x_check = x_test\n",
    "            y_check = y_test\n",
    "        else:\n",
    "            x_check = x_val\n",
    "            y_check = y_val\n",
    "        result = None\n",
    "        for i in range(0, y_check.shape[0], batch_size):\n",
    "            if transform_sparse_data:\n",
    "                x_check_batch = x_check[i:i+batch_size].toarray()\n",
    "            else:\n",
    "                x_check_batch = x_check[i:i+batch_size]\n",
    "            new_result = confusion_matrix(y_check[i:i+batch_size], classifier.predict(x_check_batch))\n",
    "            if result is None:\n",
    "                result = new_result\n",
    "            else:\n",
    "                result += new_result\n",
    "    if return_model:\n",
    "        return result, classifier\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[153,   7,   7,   2,  10],\n",
       "       [ 72,   7,   2,   1,   4],\n",
       "       [108,  11,  20,  15,  26],\n",
       "       [143,   6,  20,  48, 111],\n",
       "       [368,  20,  12,  49, 378]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment(CountVectorizer(min_df=250), GaussianNB(), summaryText, labels_multi, number_of_datapoints=10000,\n",
    "           transform_sparse_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 0.45734367971210077\n",
      "500 0.48575123706702655\n",
      "750 0.49794197031039134\n",
      "1000 0.5058816914080072\n",
      "1250 0.5070625281151597\n",
      "1500 0.5113697705802969\n",
      "1750 0.5112910481331534\n",
      "2000 0.5108187134502924\n",
      "2250 0.5068713450292398\n",
      "2500 0.506264057579847\n",
      "2750 0.50944669365722\n"
     ]
    }
   ],
   "source": [
    "# wybÃ³r modelu na zbiorze walidacyjnym wybierany pod kÄ…tem gÅ‚Ã³wnego zadania - klasyfikacji piÄ™cioklasowej\n",
    "\n",
    "results = []\n",
    "\n",
    "batch_size = 10000\n",
    "for min_df in range(250, 3000, 250):\n",
    "    res = experiment(CountVectorizer(min_df=min_df), GaussianNB(), reviewText, labels_multi,\n",
    "                     transform_sparse_data=True, batch_size=batch_size)\n",
    "    print(min_df, accuracy(res))\n",
    "    results.append((min_df, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PrÃ³ba na zbiorze testowym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = experiment(CountVectorizer(min_df=1500), GaussianNB(), reviewText, labels_multi,\n",
    "                      transform_sparse_data=True, batch_size=batch_size, use_test_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7990  1406   586   344  1594]\n",
      " [ 2939  1233   743   507  1271]\n",
      " [ 3510  1546  1950  1785  4014]\n",
      " [ 3161  1383  2041  4522 12475]\n",
      " [ 5537  1885  2187  5921 40619]]\n",
      "accuracy:  0.5066532312481444\n",
      "f1_score:  0.34310851521789626\n",
      "mae : 0.9153208755814267\n",
      "mse : 2.2640689524872015\n"
     ]
    }
   ],
   "source": [
    "print(test_res)\n",
    "print('accuracy: ', accuracy(test_res))\n",
    "print('f1_score: ', macro_f1_score(test_res))\n",
    "print('mae :', mae(test_res))\n",
    "print('mse :', mse(test_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 0.4565901934322987\n",
      "300 0.4915991902834008\n",
      "450 0.5299257759784075\n",
      "600 0.5232905982905983\n",
      "750 0.4799257759784076\n",
      "900 0.4730656770130454\n",
      "1050 0.46920827710301394\n",
      "1200 0.4626293297345929\n",
      "1350 0.45516194331983806\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "for min_df in range(150, 1500, 150):\n",
    "    res = experiment(CountVectorizer(min_df=min_df), GaussianNB(), summaryText, labels_multi,\n",
    "                     transform_sparse_data=True, batch_size=batch_size)\n",
    "    print(min_df, accuracy(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_test_res = experiment(CountVectorizer(min_df=500), GaussianNB(), summaryText, labels_multi,\n",
    "                              transform_sparse_data=True, batch_size=batch_size, use_test_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pomimo znacznie mniejszej liczby sÅ‚Ã³w, podsumwania rÃ³wnieÅ¼ sÄ… bardzo dobrÄ… podstawÄ… do predykcji (co jest oczekiwane od podsumowania). Na zbiorze testowym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6852  1305   285   301  3177]\n",
      " [ 2541  1569   494   353  1736]\n",
      " [ 2355  2772  2040  1404  4234]\n",
      " [ 2850  1707  1359  3859 13807]\n",
      " [ 4871  2287  1190  4231 43570]]\n",
      "accuracy:  0.5208323961529119\n",
      "f1_score:  0.34858359446700715\n",
      "mae : 0.9165624522037985\n",
      "mse : 2.3548839845612646\n"
     ]
    }
   ],
   "source": [
    "print(summary_test_res)\n",
    "print('accuracy: ', accuracy(summary_test_res))\n",
    "print('f1_score: ', macro_f1_score(summary_test_res))\n",
    "print('mae :', mae(summary_test_res))\n",
    "print('mse :', mse(summary_test_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Random forest </h4>\n",
    "\n",
    "Klasyfikator RandomForest powinien poradziÄ‡ sobie lepiej niÅ¼ Naiwny Bayes. Proces uczenia okazaÅ‚ siÄ™ jednak zbyt kosztowny, by wykorzystaÄ‡ peÅ‚ny zbiÃ³r danych. Klasyfikator uczony na niewielkim zbiorze i tak daje lepsze wyniki niÅ¼ GaussianNB.\n",
    "\n",
    "Powodem wysokiego kosztu obliczeniowego jest prawdopodobnie rzadki charakter danych, przez ktÃ³ry gÅ‚Ä™bokie, silnie dopasowane do zbioru uczÄ…cego drzewa caÅ‚kiem dobrze wypadajÄ… na zbiorze testowym (>60%). Ograniczenie gÅ‚Ä™bokoÅ›ci drzew skutkuje szybszym uczeniem, ale takÅ¼e znacznym spadkiem jakoÅ›ci predykcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5746875\n"
     ]
    }
   ],
   "source": [
    "res = experiment(CountVectorizer(min_df=200), RandomForestClassifier(n_estimators=200, n_jobs=-1), reviewText,\n",
    "                     labels_multi, number_of_datapoints=20000)\n",
    "print(accuracy(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5346875\n"
     ]
    }
   ],
   "source": [
    "res = experiment(CountVectorizer(min_df=500), RandomForestClassifier(n_estimators=500,  max_depth=15,\n",
    "                n_jobs=-1, min_samples_split=10), reviewText, labels_multi, number_of_datapoints=20000)\n",
    "print(accuracy(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Regresja logistyczna </h4>\n",
    "\n",
    "Skoro klasyfkator zÅ‚oÅ¼ony jest trudny do wykorzystania, konkurencyjnym modelem moÅ¼e okazaÄ‡ siÄ™ regresja logistyczna, ktÃ³ra powinna nauczyÄ‡ siÄ™ znacznie szybciej, a zwrÃ³ciÄ‡ lepsze wyniki niÅ¼ GaussianNB. Podobnie jak RandomForest, moÅ¼e korzystaÄ‡ z macierzy rzadkich, dlatego nie trzeba preztwarzaÄ‡ ich na peÅ‚ne macierze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.627519118308592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grzegorz\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# tekst recenzji\n",
    "\n",
    "res = experiment(CountVectorizer(min_df=1000), LogisticRegression(max_iter=100), reviewText, labels_multi)\n",
    "print(accuracy(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6073549257759784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grzegorz\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# podsumowanie recenzji\n",
    "\n",
    "res = experiment(CountVectorizer(min_df=100), LogisticRegression(max_iter=100), summaryText, labels_multi)\n",
    "print(accuracy(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x_train, x_test, x_train_summary, x_test_summary, y_train, y_test = train_test_split(reviewText, summaryText, labels_multi, test_size=0.2, random_state=111)\n",
    "x_train, x_val, x_train_summary, x_val_summary, y_train, y_val = train_test_split(x_train, x_train_summary, y_train, test_size=0.2, random_state=7)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZwiÄ™zÅ‚e podsumowana recenzji, choÄ‡ sÄ… bardzo krÃ³tkie, pozwalajÄ… na niemal rÃ³wnie dobrÄ… predykcjÄ™. MoÅ¼liwe, Å¼e ich poÅ‚Ä…czenie da lepsze wyniki - trudne do sklasyfikowania podsumowania mogÄ… towarzyszyÄ‡ Å‚atwym tekstom do recenzji lub na odwrÃ³t.\n",
    "\n",
    "Warto wykorzystaÄ‡ takÅ¼e informacjÄ™ o wzajemnym poÅ‚oÅ¼eniu sÅ‚Ã³w - n-gramy lub skip-gramy. Ze wzglÄ™du na duÅ¼y zbiÃ³r danych, przetwarzanie tekstÃ³w i przechowywanie ich w postaci macierzy gÄ™stych moÅ¼e byÄ‡ kosztowne (w tej postacicaÅ‚y zbiÃ³r nie mieÅ›ci siÄ™ jednoczeÅ›nie w pamiÄ™ci). Przetwarzanie tekstu powinno wiÄ™c skutkowaÄ‡ otrzymaniem macierzy rzadkiej, w miarÄ™ moÅ¼liwoÅ›ci z odrzuceniem rzadko wystÄ™pujÄ…cych (nieinformatywnych) zbitkÃ³w sÅ‚Ã³w, dlatego wykorzystana zostanie modyfkacja klasy CountVectrizer.\n",
    "\n",
    "Wykorzystana implementacja moÅ¼e zostaÄ‡ Å‚atwo uogÃ³lniona na rÃ³Å¼ne definicje skip-gramÃ³w lub n-gramÃ³w. Ze zbioru uczÄ…cego zostanÄ… wyciÄ…gniÄ™te 2-gramy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from toolz import itertoolz, compose\n",
    "except:\n",
    "    !pip install toolz\n",
    "    from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÅºrÃ³dÅ‚o: StackOverflow\n",
    "\n",
    "class SkipGramVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):    \n",
    "        preprocess = self.build_preprocessor()\n",
    "        stop_words = self.get_stop_words()\n",
    "        tokenize = self.build_tokenizer()\n",
    "        return lambda doc: self._word_skip_grams(\n",
    "                compose(tokenize, preprocess, self.decode)(doc),\n",
    "                stop_words)\n",
    "\n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "        return compose(cmap(' '.join), pluck([0, 1]), sliding_window(2))(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kowalski ukradÅ‚', 'krzesÅ‚o poszedÅ‚', 'poszedÅ‚ siedzieÄ‡', 'ukradÅ‚ krzesÅ‚o']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['Kowalski ukradÅ‚ krzesÅ‚o i poszedÅ‚ siedzieÄ‡']\n",
    "\n",
    "vect = SkipGramVectorizer()\n",
    "vect.fit(text)\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramText = SkipGramVectorizer(min_df=100)\n",
    "skipgrams = ngramText.fit_transform(reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grzegorz\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# vectorizer=None - wektoryzacja zostaÅ‚a juÅ¼ wykonana\n",
    "res = experiment(None, LogisticRegression(max_iter=100), skipgrams, labels_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6345029239766082\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramSummary = SkipGramVectorizer(min_df=100)\n",
    "skipgramsSummary = ngramSummary.fit_transform(summaryText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5707939721097616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grzegorz\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "res = experiment(None, LogisticRegression(max_iter=100), skipgramsSummary, labels_multi)\n",
    "print(accuracy(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> KoÅ„cowy model </h3>\n",
    "\n",
    "Do klasyfikacji zostanie wykorzystana regresja logistyczna z parametrami pozyskanymi z kolumn reviewText i summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "textVectorizer = CountVectorizer(min_df=1000)\n",
    "summaryVectorizer = CountVectorizer(min_df=100)\n",
    "textData = textVectorizer.fit_transform(reviewText)\n",
    "summaryData = summaryVectorizer.fit_transform(summaryText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6795096716149348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grzegorz\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "fullData = hstack([textData, summaryData, skipgrams, skipgramsSummary])\n",
    "res, model = experiment(None, LogisticRegression(max_iter=100), fullData, labels_multi, return_model=True)\n",
    "print(accuracy(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7406   755   693   206   667]\n",
      " [ 1987  1168  1522   317   457]\n",
      " [ 1030   703  4617  2205  1570]\n",
      " [  325   145  1763  7177  9139]\n",
      " [  509    96   607  3802 40054]]\n",
      "accuracy:  0.6795096716149348\n",
      "f1_score:  0.6822640928391778\n",
      "mae : 0.43360323886639673\n",
      "mse : 0.7635627530364373\n"
     ]
    }
   ],
   "source": [
    "print(res)\n",
    "print('accuracy: ', accuracy(res))\n",
    "print('f1_score: ', macro_f1_score(res))\n",
    "print('mae :', mae(res))\n",
    "print('mse :', mse(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('clfasifier.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open('vectorizers.pickle', 'wb') as f:\n",
    "    pickle.dump([textVectorizer, summaryVectorizer, ngramText, ngramSummary], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
