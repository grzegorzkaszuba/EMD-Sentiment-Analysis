{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Eksploracja masywnych danych </h1>\n",
    "<h2> Projekt Python - analiza sentymentu </h2>\n",
    "\n",
    "<h3> 1. Wstępna analiza danych i preprocessing </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(555791, 9)\n",
      "(555745, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A35C43YE9HU9CN</td>\n",
       "      <td>B0064X7B4A</td>\n",
       "      <td>Joan Miller</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I have decided not to play this game.  I can't...</td>\n",
       "      <td>Friends</td>\n",
       "      <td>1396396800</td>\n",
       "      <td>04 2, 2014</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHFS8CGWWXB5B</td>\n",
       "      <td>B00H1P4V3E</td>\n",
       "      <td>WASH ST. GAMER</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>The Amazon Appstore free app of the day for Ju...</td>\n",
       "      <td>Amazon Makes This \"Longest Spring Ever\" for Fi...</td>\n",
       "      <td>1402272000</td>\n",
       "      <td>06 9, 2014</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3EW8OTQ90NVHM</td>\n",
       "      <td>B00CLVW82O</td>\n",
       "      <td>Kindle Customer</td>\n",
       "      <td>[0, 4]</td>\n",
       "      <td>this game was so mush fun I wish I could play ...</td>\n",
       "      <td>best</td>\n",
       "      <td>1368921600</td>\n",
       "      <td>05 19, 2013</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AJ3GHFJY1IUTD</td>\n",
       "      <td>B007T9WVKM</td>\n",
       "      <td>BrawlMaster4</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>Its pretty fun and very good looking,  but you...</td>\n",
       "      <td>Fun Game</td>\n",
       "      <td>1350172800</td>\n",
       "      <td>10 14, 2012</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3JJGBS4EL603S</td>\n",
       "      <td>B00J206J5E</td>\n",
       "      <td>K. Wilson \"thesupe\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>good graphics; immersive storyline; hard to st...</td>\n",
       "      <td>great game!</td>\n",
       "      <td>1396915200</td>\n",
       "      <td>04 8, 2014</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A3RL7Y2FJBDHJ0</td>\n",
       "      <td>B006H7TC3Q</td>\n",
       "      <td>hi</td>\n",
       "      <td>[2, 5]</td>\n",
       "      <td>its very good.u use fotos on ur device and it ...</td>\n",
       "      <td>very good</td>\n",
       "      <td>1337817600</td>\n",
       "      <td>05 24, 2012</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AUHVMC0PURGO8</td>\n",
       "      <td>B006R6VG9K</td>\n",
       "      <td>A.Mccullough</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>the game is very fun and fast paced. It also k...</td>\n",
       "      <td>fun and fast paced</td>\n",
       "      <td>1401926400</td>\n",
       "      <td>06 5, 2014</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A1Z37DUIWXJNLN</td>\n",
       "      <td>B00B63HT8Q</td>\n",
       "      <td>Julie Quick \"Beach Bum Wannabe\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>great app!  A quick look at the weather... not...</td>\n",
       "      <td>A great alternative to the current (stupid) ve...</td>\n",
       "      <td>1377388800</td>\n",
       "      <td>08 25, 2013</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AF7ZE5MRM6CW2</td>\n",
       "      <td>B00BL0I7WG</td>\n",
       "      <td>T.dd</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>So fare I like it haven't had it long enough t...</td>\n",
       "      <td>easy fun</td>\n",
       "      <td>1369612800</td>\n",
       "      <td>05 27, 2013</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A1TTH51E2651BJ</td>\n",
       "      <td>B00GRXA7GG</td>\n",
       "      <td>Joni</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This classic Mahjong comes with nice graphics ...</td>\n",
       "      <td>Mahjong Premium</td>\n",
       "      <td>1394841600</td>\n",
       "      <td>03 15, 2014</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A09537361YDF89V20S3QI</td>\n",
       "      <td>B00DBGXAVG</td>\n",
       "      <td>Shronica</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>I guess its okay. Again my daughter downloaded...</td>\n",
       "      <td>I guess</td>\n",
       "      <td>1396915200</td>\n",
       "      <td>04 8, 2014</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A3PDQ6MSVNAHU5</td>\n",
       "      <td>B0064X7B4A</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>fun to play with friends from all around the w...</td>\n",
       "      <td>better than the board game!</td>\n",
       "      <td>1372377600</td>\n",
       "      <td>06 28, 2013</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AVG4YZ0ZV4F9X</td>\n",
       "      <td>B004Q6Y2LM</td>\n",
       "      <td>CDarbs</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Game ran fine on my Droid- noticed only a slig...</td>\n",
       "      <td>No gameplay problems, but not my cup of tea</td>\n",
       "      <td>1306368000</td>\n",
       "      <td>05 26, 2011</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AM55082AHY624</td>\n",
       "      <td>B008XG1X18</td>\n",
       "      <td>nicholscl</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This is a different way to look at pinterest a...</td>\n",
       "      <td>Love Pinterst on my PC</td>\n",
       "      <td>1379462400</td>\n",
       "      <td>09 18, 2013</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A2ZC0Q9GKBD44E</td>\n",
       "      <td>B008JJS6D2</td>\n",
       "      <td>Caitlin Anne \"Caitling\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This is one of the most fun games, I have play...</td>\n",
       "      <td>fun</td>\n",
       "      <td>1358553600</td>\n",
       "      <td>01 19, 2013</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               reviewerID        asin                     reviewerName  \\\n",
       "0          A35C43YE9HU9CN  B0064X7B4A                      Joan Miller   \n",
       "1           AHFS8CGWWXB5B  B00H1P4V3E                   WASH ST. GAMER   \n",
       "2          A3EW8OTQ90NVHM  B00CLVW82O                  Kindle Customer   \n",
       "3           AJ3GHFJY1IUTD  B007T9WVKM                     BrawlMaster4   \n",
       "4          A3JJGBS4EL603S  B00J206J5E              K. Wilson \"thesupe\"   \n",
       "5          A3RL7Y2FJBDHJ0  B006H7TC3Q                               hi   \n",
       "6           AUHVMC0PURGO8  B006R6VG9K                     A.Mccullough   \n",
       "7          A1Z37DUIWXJNLN  B00B63HT8Q  Julie Quick \"Beach Bum Wannabe\"   \n",
       "8           AF7ZE5MRM6CW2  B00BL0I7WG                             T.dd   \n",
       "9          A1TTH51E2651BJ  B00GRXA7GG                             Joni   \n",
       "10  A09537361YDF89V20S3QI  B00DBGXAVG                         Shronica   \n",
       "11         A3PDQ6MSVNAHU5  B0064X7B4A                  Amazon Customer   \n",
       "12          AVG4YZ0ZV4F9X  B004Q6Y2LM                           CDarbs   \n",
       "13          AM55082AHY624  B008XG1X18                        nicholscl   \n",
       "14         A2ZC0Q9GKBD44E  B008JJS6D2          Caitlin Anne \"Caitling\"   \n",
       "\n",
       "   helpful                                         reviewText  \\\n",
       "0   [0, 0]  I have decided not to play this game.  I can't...   \n",
       "1   [3, 4]  The Amazon Appstore free app of the day for Ju...   \n",
       "2   [0, 4]  this game was so mush fun I wish I could play ...   \n",
       "3   [0, 2]  Its pretty fun and very good looking,  but you...   \n",
       "4   [0, 0]  good graphics; immersive storyline; hard to st...   \n",
       "5   [2, 5]  its very good.u use fotos on ur device and it ...   \n",
       "6   [0, 0]  the game is very fun and fast paced. It also k...   \n",
       "7   [0, 0]  great app!  A quick look at the weather... not...   \n",
       "8   [0, 0]  So fare I like it haven't had it long enough t...   \n",
       "9   [0, 0]  This classic Mahjong comes with nice graphics ...   \n",
       "10  [1, 1]  I guess its okay. Again my daughter downloaded...   \n",
       "11  [0, 0]  fun to play with friends from all around the w...   \n",
       "12  [0, 0]  Game ran fine on my Droid- noticed only a slig...   \n",
       "13  [0, 0]  This is a different way to look at pinterest a...   \n",
       "14  [0, 0]  This is one of the most fun games, I have play...   \n",
       "\n",
       "                                              summary  unixReviewTime  \\\n",
       "0                                             Friends      1396396800   \n",
       "1   Amazon Makes This \"Longest Spring Ever\" for Fi...      1402272000   \n",
       "2                                                best      1368921600   \n",
       "3                                            Fun Game      1350172800   \n",
       "4                                         great game!      1396915200   \n",
       "5                                           very good      1337817600   \n",
       "6                                  fun and fast paced      1401926400   \n",
       "7   A great alternative to the current (stupid) ve...      1377388800   \n",
       "8                                            easy fun      1369612800   \n",
       "9                                     Mahjong Premium      1394841600   \n",
       "10                                            I guess      1396915200   \n",
       "11                        better than the board game!      1372377600   \n",
       "12        No gameplay problems, but not my cup of tea      1306368000   \n",
       "13                             Love Pinterst on my PC      1379462400   \n",
       "14                                                fun      1358553600   \n",
       "\n",
       "     reviewTime  score  \n",
       "0    04 2, 2014    1.0  \n",
       "1    06 9, 2014    2.0  \n",
       "2   05 19, 2013    5.0  \n",
       "3   10 14, 2012    5.0  \n",
       "4    04 8, 2014    5.0  \n",
       "5   05 24, 2012    5.0  \n",
       "6    06 5, 2014    5.0  \n",
       "7   08 25, 2013    4.0  \n",
       "8   05 27, 2013    5.0  \n",
       "9   03 15, 2014    5.0  \n",
       "10   04 8, 2014    3.0  \n",
       "11  06 28, 2013    5.0  \n",
       "12  05 26, 2011    3.0  \n",
       "13  09 18, 2013    3.0  \n",
       "14  01 19, 2013    4.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('reviews_train.csv')\n",
    "print(df.shape)\n",
    "df = df.dropna()\n",
    "print(df.shape)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score\n",
       "1.0    0.108215\n",
       "2.0    0.060702\n",
       "3.0    0.114966\n",
       "4.0    0.209690\n",
       "5.0    0.506427\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_counts = df.value_counts('score')\n",
    "score_frequencies = score_counts/np.sum(score_counts)\n",
    "score_frequencies = score_frequencies.sort_index()\n",
    "score_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Interpretacja danych </h4>\n",
    "\n",
    "Wstępne przejrzenie przykładowych danych dostępnych w dokumencie pozwala na wstępną ocenę ich użyteczności, a także potencjalnych trudności w ich interpretacji.\n",
    "\n",
    "Już pierwszy rzut oka na dane wskazuje, jak wielkie znaczenie może mieć (przeważnie krótkie) podsumowanie obecne w kolumnie \"summary\". Niektórzy użytkownicy wprost umeszczają w niej swoje oceny, łatwe do zinterpretowania na kilkustopniowej skali.\n",
    "\n",
    "Najbardziej wiarygodną podstawą do klasyfikacji, pod warunkiem dobrej jakości modelu, powinien być pełen tekst recenzji. W przypadku problemu analizy sentymentu, kwestia wykorzystania \"stop words\" jest trudna. Wiele słów (np. zaprzeczeń) są kluczowe dla prawidłowej klasyfikacji wypowiedzi i nie należy ich usuwać. Być może jednak warto pozbyć się innych, nie niosących takich informacji.\n",
    "\n",
    "Wiele słów wprost odwraca znaczenie dalszej części zdania. Nie sa to tylko trywialne przypadki, jak te z zaprzeczeniem (np. \"not\"). Już w niewielkim wycinku danych widoczny jest taki przykład - \"A great ALTERNATIVE to (...) STUPID...\". Pokazuje to, że słowa, które w ten sposób działają na interpretację zdania są powszechne i nietrywialne - odpowiednio dobrany system uczący powinien zwracać uwagę na kolejność słów lub choćby rozróżniać kolejne zdania. By sprawdzić, jak ważna jest ta zdolność, najpierw zostanie przeprowadzona klasyfikacja z użyciem modelu, który jej nie posiada - naiwnego klasyfikatora bayesowskiego.\n",
    "\n",
    "Wykorzystanie głosów innych użytkowników na temat użyteczności opinii może mijać się z celem. Na podstawie niewielkej próbki zdaje się, że liczba głosów pod opiniami wynosi zero lub jest niewielka. Można się spodziewać, że pozytywne głosy zbiorą te recenzję, których ocena zgodna jest z oceną większości użytkowników, a opinie niepopularne zbiorą głosy negatywne. Obecne w zbiorze opinie dotyczą jednak różnych produktów, wobec tego prawdopodobnie nie ma sensu śledzić tego ogólnego trendu, by wydobyć dodatkowe informacje.\n",
    "\n",
    "Problemem, który należy mieć na uwadze, jest niezbilansowanie zbioru pod względem ocen. Ponad połowa ocen jest równa 5, a ponad 70% jest równa 4 lub 5. Można więc przyjąć, że klasyfikatory, które wnoszą pewną jakość do klasyfikacji (oczywiście z perspektywy trafności, a nie innych metryk) to takie, które klasyfikują prawidłowo więcej niż 50% przypadków.\n",
    "\n",
    "Pierwsze podejście do problemu zostanie wykonane w dwóch wariantach - z predykcją jednej z 5 ocen lub predykcją, czy ocena jest pozytywna (4-5) lub negatywna (1-3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A35C43YE9HU9CN</td>\n",
       "      <td>B0064X7B4A</td>\n",
       "      <td>Joan Miller</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I have decided not to play this game.  I can't...</td>\n",
       "      <td>Friends</td>\n",
       "      <td>1396396800</td>\n",
       "      <td>04 2, 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHFS8CGWWXB5B</td>\n",
       "      <td>B00H1P4V3E</td>\n",
       "      <td>WASH ST. GAMER</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>The Amazon Appstore free app of the day for Ju...</td>\n",
       "      <td>Amazon Makes This \"Longest Spring Ever\" for Fi...</td>\n",
       "      <td>1402272000</td>\n",
       "      <td>06 9, 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3EW8OTQ90NVHM</td>\n",
       "      <td>B00CLVW82O</td>\n",
       "      <td>Kindle Customer</td>\n",
       "      <td>[0, 4]</td>\n",
       "      <td>this game was so mush fun I wish I could play ...</td>\n",
       "      <td>best</td>\n",
       "      <td>1368921600</td>\n",
       "      <td>05 19, 2013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AJ3GHFJY1IUTD</td>\n",
       "      <td>B007T9WVKM</td>\n",
       "      <td>BrawlMaster4</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>Its pretty fun and very good looking,  but you...</td>\n",
       "      <td>Fun Game</td>\n",
       "      <td>1350172800</td>\n",
       "      <td>10 14, 2012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3JJGBS4EL603S</td>\n",
       "      <td>B00J206J5E</td>\n",
       "      <td>K. Wilson \"thesupe\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>good graphics; immersive storyline; hard to st...</td>\n",
       "      <td>great game!</td>\n",
       "      <td>1396915200</td>\n",
       "      <td>04 8, 2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin         reviewerName helpful  \\\n",
       "0  A35C43YE9HU9CN  B0064X7B4A          Joan Miller  [0, 0]   \n",
       "1   AHFS8CGWWXB5B  B00H1P4V3E       WASH ST. GAMER  [3, 4]   \n",
       "2  A3EW8OTQ90NVHM  B00CLVW82O      Kindle Customer  [0, 4]   \n",
       "3   AJ3GHFJY1IUTD  B007T9WVKM         BrawlMaster4  [0, 2]   \n",
       "4  A3JJGBS4EL603S  B00J206J5E  K. Wilson \"thesupe\"  [0, 0]   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  I have decided not to play this game.  I can't...   \n",
       "1  The Amazon Appstore free app of the day for Ju...   \n",
       "2  this game was so mush fun I wish I could play ...   \n",
       "3  Its pretty fun and very good looking,  but you...   \n",
       "4  good graphics; immersive storyline; hard to st...   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0                                            Friends      1396396800   \n",
       "1  Amazon Makes This \"Longest Spring Ever\" for Fi...      1402272000   \n",
       "2                                               best      1368921600   \n",
       "3                                           Fun Game      1350172800   \n",
       "4                                        great game!      1396915200   \n",
       "\n",
       "    reviewTime  score  \n",
       "0   04 2, 2014      0  \n",
       "1   06 9, 2014      0  \n",
       "2  05 19, 2013      1  \n",
       "3  10 14, 2012      1  \n",
       "4   04 8, 2014      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_grade_df = df.copy()\n",
    "binary_grade_df['score'] = np.where(binary_grade_df['score'] > 3, 1, 0)\n",
    "binary_grade_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Grzegorz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Grzegorz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have decided not to play this game.  I can't keep track of everyone, or play often enough to make it fun.\n",
      "I have decided not to play this game . I ca n't keep track of everyone , or play often enough to make it fun .\n",
      "I have decided not to play this game . I ca n't keep track of everyone , or play often enough to make it fun .\n",
      "i have decid not to play thi game . i ca n't keep track of everyon , or play often enough to make it fun .\n"
     ]
    }
   ],
   "source": [
    "example_sentence = binary_grade_df.iloc[0,4]\n",
    "tokens = word_tokenize(binary_grade_df.iloc[0,4])\n",
    "tokens_pretty = ' '.join(tokens)\n",
    "lemmas = ' '.join([lemmatizer.lemmatize(t) for t in tokens])\n",
    "stems = ' '.join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "print(example_sentence, tokens_pretty, lemmas, stems, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać na powyższym przykładzie, stemming upraszcza tekst nieco bardziej niż lematyzacja. Z jednej strony, wpływa to na lepsze uogólnienie słów, z drugiej - może zniekształcić kontekst zdania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2. Wstępna klasyfikacja </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ze względu na rodzaj problemu (kolejnym etykietom odpowiadają wartości uporządkowane liniowo), oprócz metryk typowych dla klasyfikacji wieloklasowej, wykorzystane zostaną metryki typowe dla regresji. W poniższej kolumnie zdefiniowano uogólnienie metryk f1 dla klasyfikacji wieloklasowej oraz średni błąd absolutny i błąd średniokwadratowy, wyliczane z macierzy pomyłek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = lambda confusion_mx, clazz: confusion_mx[clazz, clazz]/np.sum(confusion_mx[:, clazz])\n",
    "precision = lambda confusion_mx, clazz: confusion_mx[clazz, clazz]/np.sum(confusion_mx[clazz, :])\n",
    "micro_f1_score = lambda confusion_mx, clazz: 2/(precision(confusion_mx, clazz)**-1 * recall(confusion_mx, clazz)**-1)\n",
    "macro_f1_score = lambda confusion_mx: np.mean([micro_f1_score(confusion_mx, clazz) for clazz in range(confusion_mx.shape[0])])\n",
    "\n",
    "accuracy = lambda confusion_mx: np.sum(np.diag(confusion_mx))/np.sum(confusion_mx)\n",
    "\n",
    "error_matrix = np.abs(np.arange(5)[:, np.newaxis] - np.arange(5))\n",
    "mae = lambda confusion_mx: np.sum(confusion_mx * error_matrix / np.sum(confusion_mx))\n",
    "mse = lambda confusion_mx: np.sum(confusion_mx * error_matrix**2 /  np.sum(confusion_mx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gdyby klasyfikator losowo przypisywał etykiety, ale z prawdopodobieństwem proporcjonalnym do liczebności klasy decyzyjnej, wówczas f1 score wyniósłby:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13162005051281517"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "dummy_confusion_mx = lambda freqs, probs: freqs[:, np.newaxis] * probs\n",
    "\n",
    "freqs = np.array(score_frequencies)\n",
    "probs = freqs\n",
    "\n",
    "macro_f1_score(dummy_confusion_mx(freqs, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla wygody modelowania wyników klasyfikatora niewnoszącego wartości informacyjnej, a także późniejszego przetwarzania licznych wyników z walidacji jedynie na podstawie macierzy pomyłek, zdefiniowano funkcje accuracy i f1_score wyliczane z macierzy pomyłek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "reviewText = np.array(df['reviewText'])\n",
    "summaryText = np.array(df['summary'])\n",
    "labels_multi = np.array(df['score'])\n",
    "labels_binary = np.array(binary_grade_df['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Wstępna optymalizacja modelu </h4>\n",
    "\n",
    "Zaproponowano dwa sposoby wektoryzacji, z wykorzystaniem klasy CountVectorizer (zliczanie słów) oraz TfidfVectorizer (przypisującym wyższą wagę za wystąpienie termów wówczas, gdy znajdą się one w mniejszej liczbie przykładów ze zbioru - realizuje to podobną ideę, co wycinanie stop words, tylko w bardziej \"miękki\" sposób). <br>\n",
    "Zarówno dla jakości, jak i prędkości predykcji, konieczne okazało się ograniczenie liczby atrybutów: narzucenie limitu maksymalnej liczby najczęściej występujących słów lub minimalną liczbę wystąpień słowa w zborze danych. W drugim podejściu, optymalizacja okazała się łatwiejsza. <br>\n",
    "Zgodnie ze wskazówkami dotyczącymi problemu analizy sentymentu, lepsze wyniki osiągnięto w podejściu bez usuwania stop words. <br>\n",
    "Ze względu na znaczny koszt tokenizacji masywnego zbioru (nie jest to operacja, którą można zwektoryzować, a wyjściowe dane mają typ \"object\"), pozostano przy natywnej tokenizacji wykorzystywanej przez pakiet sklearn.\n",
    "\n",
    "Zoptymalizowany model osiąga lepsze wyniki niż \"ślepy\" klasyfikator, preferujący zawsze klasę większościową. Niezoptymalizowane modele daleko odbiegały od tych wyników - w obecności zbędnych, mało informatywnych słów, klasyfikacja była znacznie zaburzona i klasa większościowa nie zawsze nawet tą najczęściej wybieraną przez model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer1 = CountVectorizer(min_df=250)\n",
    "vectorizer2 = TfidfVectorizer(min_df=250)\n",
    "#vectorizer1 = CountVectorizer(min_df=250, stop_words='english')\n",
    "#vectorizer2 = TfidfVectorizer(min_df=250, stop_words='english')\n",
    "datapoints = 50000\n",
    "xdata1 = vectorizer1.fit_transform(reviewText[:datapoints])\n",
    "xdata2 = vectorizer2.fit_transform(reviewText[:datapoints])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Dobór vectorizera </h4>\n",
    "\n",
    "Ze względu na znaczną ilość danych, wstępny wybór modelu został wykonany przez iteracyjny dobór parametru min_df i porównanie działania obu klas Vectorizer. Ponieważ próbka 50000 punktów danych to nieznaczna część całego zbioru (mniej niż 10%), nie wydzielono zbioru testowego. Zbiór walidacyjny i testowy zostaną rozróżnione przy finalnej parametryzacji modelu dla całego datasetu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_val1, X_train2, X_val2, y_train, y_val, binary_y_train, binary_y_val = train_test_split(\n",
    "    xdata1, xdata2, df['score'][:datapoints], binary_grade_df['score'][:datapoints], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classifier1 = GaussianNB()\n",
    "binary_classifier2 = GaussianNB()\n",
    "multiclass_classifier1 = GaussianNB()\n",
    "multiclass_classifier2 = GaussianNB()\n",
    "\n",
    "binary_classifier1.fit(X_train1.toarray(), binary_y_train)\n",
    "binary_classifier2.fit(X_train2.toarray(), binary_y_train)\n",
    "multiclass_classifier1.fit(X_train1.toarray(), y_train)\n",
    "multiclass_classifier2.fit(X_train2.toarray(), y_train)\n",
    "\n",
    "y_pred_bin1 = binary_classifier1.predict(X_val1.toarray())\n",
    "y_pred_bin2 = binary_classifier2.predict(X_val2.toarray())\n",
    "y_pred_multi1 = multiclass_classifier1.predict(X_val1.toarray())\n",
    "y_pred_multi2 = multiclass_classifier2.predict(X_val2.toarray())\n",
    " \n",
    "res_bin_1 = confusion_matrix(binary_y_val, y_pred_bin1)\n",
    "res_bin_2 = confusion_matrix(binary_y_val, y_pred_bin2)\n",
    "res_multi_1 = confusion_matrix(y_val, y_pred_multi1)\n",
    "res_multi_2 = confusion_matrix(y_val, y_pred_multi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorizer accuracies:  0.7656 0.4815\n",
      "Count vectorizer accuracies:  0.7485 0.4456\n"
     ]
    }
   ],
   "source": [
    "print('Count vectorizer accuracies: ', accuracy(res_bin_1), accuracy(res_multi_1))\n",
    "print('Count vectorizer accuracies: ', accuracy(res_bin_2), accuracy(res_multi_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer odnosi nieco lepsze rezultaty na zbiorze walidacyjnym. Zostanie wykorzystany w dalszej części eksperymentów.\n",
    "\n",
    "<h4> Uogólnienie testów </h4>\n",
    "\n",
    "Ze względu na fakt, że klasyfikator nie jest w stanie przetwarzać wektorów rzadkich, dane uczące mogą mieć rozmiar wielokrotne przekraczający dostępną pamięć operacyjną. Zaletą staje się tutaj prostota klasyfikatora - uczy się on przez zliczanie, jednorazowo przechodząc po danych. Wystarczy zatem wykonać jedną iterację, przetwarzając kolejne fragmenty zbioru na macierz pełną.\n",
    "\n",
    "Ponadto, schemat każdego eksperymentu z wektoryzacją i klasyfikacją tekstów będzie wyglądał podobnie, jak w przykładzie powyżej. Czas zdefiniować całą procedurę. Potrzebna będzie możliwość doboru różnych wektoryzacji, różnych klasyfikatorów, różnych danych i etykiet (binarnych lub wieloklasowych), a także wybór między wektorem danych w postaci pełnej i rzadkiej. Drobna nadmiarowość (na przykład każdorazowe wyznaczanie trzech zbiorów - uczącego, walidacyjnego i testowego) zostanie zrekompensowana przez wygodę użycia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(vectorizer, classifier, data, labels, number_of_datapoints=None, batch_size=None, use_test_set=False, transform_sparse_data=False, return_model=False):\n",
    "    \n",
    "    if vectorizer is not None:\n",
    "        # Wektoryzacja - eksperyment na niepełnym zbiorze danych\n",
    "        if number_of_datapoints is None:\n",
    "            xdata = vectorizer.fit_transform(data)\n",
    "        # ...na pełnym zbiorze\n",
    "        else:\n",
    "            xdata = vectorizer.fit_transform(data[:number_of_datapoints])\n",
    "            labels = labels[:number_of_datapoints]\n",
    "    else:\n",
    "        if number_of_datapoints is None:\n",
    "            xdata = data\n",
    "        else:\n",
    "            xdata = vectorizer[:number_of_datapoints]\n",
    "            labels = labels[:number_of_datapoints]\n",
    "        \n",
    "    # deterministyczny podział na zbiór uczący, walidacyjny i testowy\n",
    "    x_train, x_test, y_train, y_test = train_test_split(xdata, labels, test_size=0.2, random_state=111)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=7)\n",
    "    \n",
    "    # uczenie i walidacja/testowanie\n",
    "    if batch_size is None:\n",
    "        # wymagane przez klasyfikator NB, niewymagane przez regresję logistyczną\n",
    "        if transform_sparse_data:\n",
    "            x_train = x_train.toarray()\n",
    "            x_val = x_val.toarray()\n",
    "            x_test = x_test.toarray()\n",
    "        classifier.fit(x_train, y_train)\n",
    "        # ostateczna próba będzie wykonywana na zbiorze testowym\n",
    "        if use_test_set:\n",
    "            result = confusion_matrix(y_val, classifier.predict(x_val))\n",
    "        # większość (domyślnie) na walidacyjnym\n",
    "        else:\n",
    "            result = confusion_matrix(y_val, classifier.predict(x_val))\n",
    "    # wersja z batch_size potrzebuje etykiet klas do i wykorzystuje metodę partial_fit\n",
    "    else:\n",
    "        classes = np.unique(y_train)\n",
    "        for i in range(0, y_train.shape[0], batch_size):\n",
    "            if transform_sparse_data:\n",
    "                x_train_batch = x_train[i:i+batch_size].toarray()\n",
    "            else:\n",
    "                x_train_batch = x_train[i:i+batch_size]\n",
    "            classifier.partial_fit(x_train_batch, y_train[i:i+batch_size], classes)\n",
    "        if use_test_set:\n",
    "            x_check = x_test\n",
    "            y_check = y_test\n",
    "        else:\n",
    "            x_check = x_val\n",
    "            y_check = y_val\n",
    "        result = None\n",
    "        for i in range(0, y_check.shape[0], batch_size):\n",
    "            if transform_sparse_data:\n",
    "                x_check_batch = x_check[i:i+batch_size].toarray()\n",
    "            else:\n",
    "                x_check_batch = x_check[i:i+batch_size]\n",
    "            new_result = confusion_matrix(y_check[i:i+batch_size], classifier.predict(x_check_batch))\n",
    "            if result is None:\n",
    "                result = new_result\n",
    "            else:\n",
    "                result += new_result\n",
    "    if return_model:\n",
    "        return result, classifier\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[153,   7,   7,   2,  10],\n",
       "       [ 72,   7,   2,   1,   4],\n",
       "       [108,  11,  20,  15,  26],\n",
       "       [143,   6,  20,  48, 111],\n",
       "       [368,  20,  12,  49, 378]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment(CountVectorizer(min_df=250), GaussianNB(), summaryText, labels_multi, number_of_datapoints=10000,\n",
    "           transform_sparse_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 0.45734367971210077\n",
      "500 0.48575123706702655\n",
      "750 0.49794197031039134\n",
      "1000 0.5058816914080072\n",
      "1250 0.5070625281151597\n",
      "1500 0.5113697705802969\n",
      "1750 0.5112910481331534\n",
      "2000 0.5108187134502924\n",
      "2250 0.5068713450292398\n",
      "2500 0.506264057579847\n",
      "2750 0.50944669365722\n"
     ]
    }
   ],
   "source": [
    "# wybór modelu na zbiorze walidacyjnym wybierany pod kątem głównego zadania - klasyfikacji pięcioklasowej\n",
    "\n",
    "results = []\n",
    "\n",
    "batch_size = 10000\n",
    "for min_df in range(250, 3000, 250):\n",
    "    res = experiment(CountVectorizer(min_df=min_df), GaussianNB(), reviewText, labels_multi,\n",
    "                     transform_sparse_data=True, batch_size=batch_size)\n",
    "    print(min_df, accuracy(res))\n",
    "    results.append((min_df, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Próba na zbiorze testowym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = experiment(CountVectorizer(min_df=1500), GaussianNB(), reviewText, labels_multi,\n",
    "                      transform_sparse_data=True, batch_size=batch_size, use_test_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7990  1406   586   344  1594]\n",
      " [ 2939  1233   743   507  1271]\n",
      " [ 3510  1546  1950  1785  4014]\n",
      " [ 3161  1383  2041  4522 12475]\n",
      " [ 5537  1885  2187  5921 40619]]\n",
      "accuracy:  0.5066532312481444\n",
      "f1_score:  0.34310851521789626\n",
      "mae : 0.9153208755814267\n",
      "mse : 2.2640689524872015\n"
     ]
    }
   ],
   "source": [
    "print(test_res)\n",
    "print('accuracy: ', accuracy(test_res))\n",
    "print('f1_score: ', macro_f1_score(test_res))\n",
    "print('mae :', mae(test_res))\n",
    "print('mse :', mse(test_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 0.4565901934322987\n",
      "300 0.4915991902834008\n",
      "450 0.5299257759784075\n",
      "600 0.5232905982905983\n",
      "750 0.4799257759784076\n",
      "900 0.4730656770130454\n",
      "1050 0.46920827710301394\n",
      "1200 0.4626293297345929\n",
      "1350 0.45516194331983806\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "for min_df in range(150, 1500, 150):\n",
    "    res = experiment(CountVectorizer(min_df=min_df), GaussianNB(), summaryText, labels_multi,\n",
    "                     transform_sparse_data=True, batch_size=batch_size)\n",
    "    print(min_df, accuracy(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_test_res = experiment(CountVectorizer(min_df=500), GaussianNB(), summaryText, labels_multi,\n",
    "                              transform_sparse_data=True, batch_size=batch_size, use_test_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pomimo znacznie mniejszej liczby słów, podsumwania również są bardzo dobrą podstawą do predykcji (co jest oczekiwane od podsumowania). Na zbiorze testowym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6852  1305   285   301  3177]\n",
      " [ 2541  1569   494   353  1736]\n",
      " [ 2355  2772  2040  1404  4234]\n",
      " [ 2850  1707  1359  3859 13807]\n",
      " [ 4871  2287  1190  4231 43570]]\n",
      "accuracy:  0.5208323961529119\n",
      "f1_score:  0.34858359446700715\n",
      "mae : 0.9165624522037985\n",
      "mse : 2.3548839845612646\n"
     ]
    }
   ],
   "source": [
    "print(summary_test_res)\n",
    "print('accuracy: ', accuracy(summary_test_res))\n",
    "print('f1_score: ', macro_f1_score(summary_test_res))\n",
    "print('mae :', mae(summary_test_res))\n",
    "print('mse :', mse(summary_test_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Random forest </h4>\n",
    "\n",
    "Klasyfikator RandomForest powinien poradzić sobie lepiej niż Naiwny Bayes. Proces uczenia okazał się jednak zbyt kosztowny, by wykorzystać pełny zbiór danych. Klasyfikator uczony na niewielkim zbiorze i tak daje lepsze wyniki niż GaussianNB.\n",
    "\n",
    "Powodem wysokiego kosztu obliczeniowego jest prawdopodobnie rzadki charakter danych, przez który głębokie, silnie dopasowane do zbioru uczącego drzewa całkiem dobrze wypadają na zbiorze testowym (>60%). Ograniczenie głębokości drzew skutkuje szybszym uczeniem, ale także znacznym spadkiem jakości predykcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5746875\n"
     ]
    }
   ],
   "source": [
    "res = experiment(CountVectorizer(min_df=200), RandomForestClassifier(n_estimators=200, n_jobs=-1), reviewText,\n",
    "                     labels_multi, number_of_datapoints=20000)\n",
    "print(accuracy(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5346875\n"
     ]
    }
   ],
   "source": [
    "res = experiment(CountVectorizer(min_df=500), RandomForestClassifier(n_estimators=500,  max_depth=15,\n",
    "                n_jobs=-1, min_samples_split=10), reviewText, labels_multi, number_of_datapoints=20000)\n",
    "print(accuracy(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Regresja logistyczna </h4>\n",
    "\n",
    "Skoro klasyfkator złożony jest trudny do wykorzystania, konkurencyjnym modelem może okazać się regresja logistyczna, która powinna nauczyć się znacznie szybciej, a zwrócić lepsze wyniki niż GaussianNB. Podobnie jak RandomForest, może korzystać z macierzy rzadkich, dlatego nie trzeba preztwarzać ich na pełne macierze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.627519118308592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grzegorz\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# tekst recenzji\n",
    "\n",
    "res = experiment(CountVectorizer(min_df=1000), LogisticRegression(max_iter=100), reviewText, labels_multi)\n",
    "print(accuracy(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6073549257759784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grzegorz\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# podsumowanie recenzji\n",
    "\n",
    "res = experiment(CountVectorizer(min_df=100), LogisticRegression(max_iter=100), summaryText, labels_multi)\n",
    "print(accuracy(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x_train, x_test, x_train_summary, x_test_summary, y_train, y_test = train_test_split(reviewText, summaryText, labels_multi, test_size=0.2, random_state=111)\n",
    "x_train, x_val, x_train_summary, x_val_summary, y_train, y_val = train_test_split(x_train, x_train_summary, y_train, test_size=0.2, random_state=7)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwięzłe podsumowana recenzji, choć są bardzo krótkie, pozwalają na niemal równie dobrą predykcję. Możliwe, że ich połączenie da lepsze wyniki - trudne do sklasyfikowania podsumowania mogą towarzyszyć łatwym tekstom do recenzji lub na odwrót.\n",
    "\n",
    "Warto wykorzystać także informację o wzajemnym położeniu słów - n-gramy lub skip-gramy. Ze względu na duży zbiór danych, przetwarzanie tekstów i przechowywanie ich w postaci macierzy gęstych może być kosztowne (w tej postacicały zbiór nie mieści się jednocześnie w pamięci). Przetwarzanie tekstu powinno więc skutkować otrzymaniem macierzy rzadkiej, w miarę możliwości z odrzuceniem rzadko występujących (nieinformatywnych) zbitków słów, dlatego wykorzystana zostanie modyfkacja klasy CountVectrizer.\n",
    "\n",
    "Wykorzystana implementacja może zostać łatwo uogólniona na różne definicje skip-gramów lub n-gramów. Ze zbioru uczącego zostaną wyciągnięte 2-gramy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from toolz import itertoolz, compose\n",
    "except:\n",
    "    !pip install toolz\n",
    "    from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# źródło: StackOverflow\n",
    "\n",
    "class SkipGramVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):    \n",
    "        preprocess = self.build_preprocessor()\n",
    "        stop_words = self.get_stop_words()\n",
    "        tokenize = self.build_tokenizer()\n",
    "        return lambda doc: self._word_skip_grams(\n",
    "                compose(tokenize, preprocess, self.decode)(doc),\n",
    "                stop_words)\n",
    "\n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "        return compose(cmap(' '.join), pluck([0, 1]), sliding_window(2))(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kowalski ukradł', 'krzesło poszedł', 'poszedł siedzieć', 'ukradł krzesło']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['Kowalski ukradł krzesło i poszedł siedzieć']\n",
    "\n",
    "vect = SkipGramVectorizer()\n",
    "vect.fit(text)\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramText = SkipGramVectorizer(min_df=100)\n",
    "skipgrams = ngramText.fit_transform(reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grzegorz\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# vectorizer=None - wektoryzacja została już wykonana\n",
    "res = experiment(None, LogisticRegression(max_iter=100), skipgrams, labels_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6345029239766082\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramSummary = SkipGramVectorizer(min_df=100)\n",
    "skipgramsSummary = ngramSummary.fit_transform(summaryText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5707939721097616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grzegorz\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "res = experiment(None, LogisticRegression(max_iter=100), skipgramsSummary, labels_multi)\n",
    "print(accuracy(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Końcowy model </h3>\n",
    "\n",
    "Do klasyfikacji zostanie wykorzystana regresja logistyczna z parametrami pozyskanymi z kolumn reviewText i summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "textVectorizer = CountVectorizer(min_df=1000)\n",
    "summaryVectorizer = CountVectorizer(min_df=100)\n",
    "textData = textVectorizer.fit_transform(reviewText)\n",
    "summaryData = summaryVectorizer.fit_transform(summaryText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6795096716149348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grzegorz\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "fullData = hstack([textData, summaryData, skipgrams, skipgramsSummary])\n",
    "res, model = experiment(None, LogisticRegression(max_iter=100), fullData, labels_multi, return_model=True)\n",
    "print(accuracy(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7406   755   693   206   667]\n",
      " [ 1987  1168  1522   317   457]\n",
      " [ 1030   703  4617  2205  1570]\n",
      " [  325   145  1763  7177  9139]\n",
      " [  509    96   607  3802 40054]]\n",
      "accuracy:  0.6795096716149348\n",
      "f1_score:  0.6822640928391778\n",
      "mae : 0.43360323886639673\n",
      "mse : 0.7635627530364373\n"
     ]
    }
   ],
   "source": [
    "print(res)\n",
    "print('accuracy: ', accuracy(res))\n",
    "print('f1_score: ', macro_f1_score(res))\n",
    "print('mae :', mae(res))\n",
    "print('mse :', mse(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('clfasifier.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open('vectorizers.pickle', 'wb') as f:\n",
    "    pickle.dump([textVectorizer, summaryVectorizer, ngramText, ngramSummary], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
